{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python NLTK library demo\n",
    "\n",
    "### First Import Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at an example Invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "raw_invoice_image_path = 'invoice.jpg'\n",
    "raw_invoice_image = Image.open(raw_invoice_image_path)\n",
    "raw_invoice_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Some OCR text scraping, we assume data is in an unformatted text file.\n",
    "\n",
    "### Here I've manually created a file, OCR is outside of the scope of this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_image_path = 'ocr_scraped.png'\n",
    "scraped_image = Image.open(scraped_image_path)\n",
    "scraped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets read the text file and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "    file = open(path, \"r\")\n",
    "    text = file.read()\n",
    "    #print(\"Text = \", text)\n",
    "    return text\n",
    "\n",
    "#open file and tokenize the words\n",
    "text = readFile('scraped_text.txt')\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets clean the data by removing punctuation, stop words, and very short words.  In this case I've also chosen to remove tokens that are only numerical, which may or may not be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize with regular expression that removes symbols\n",
    "def tokenize_alpha_num(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens                            \n",
    "                                \n",
    "#remove stopwords\n",
    "def remove_stop_words(tokens):\n",
    "    filteredTokens=[]\n",
    "    swords = set(stopwords.words('english'))\n",
    "    for t in tokens:\n",
    "        if t not in swords:\n",
    "            filteredTokens.append(t)\n",
    "    return filteredTokens\n",
    "\n",
    "#remove tokens below n characters\n",
    "def remove_short_words(tokens, min):\n",
    "    filteredTokens = []\n",
    "    for t in tokens:\n",
    "        if len(t) >= min:\n",
    "            filteredTokens.append(t)\n",
    "    return filteredTokens\n",
    "\n",
    "#remove numbers\n",
    "def remove_digits(tokens):\n",
    "    filteredTokens = []\n",
    "    for t in tokens:\n",
    "        if not t.isdigit():\n",
    "            filteredTokens.append(t)\n",
    "    return filteredTokens\n",
    "\n",
    "tokens = tokenize_alpha_num(text)\n",
    "tokens = remove_stop_words(tokens)\n",
    "tokens = remove_short_words(tokens, 3)\n",
    "tokens = remove_digits(tokens)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we have an opportunity to use machine learning to recognize tokens as items that are relevant to the customer's business.  For example, we could identify tokens in the format PXXXX as Product IDs in our system.\n",
    "\n",
    "### NOTE that I'm not really doing machine learning here, only faking it.\n",
    "\n",
    "### To simulate this, I'll cheat and use simple functions to replace product numbers with 'PRODUCTID' and an Invoice number with INVOICEID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_product_id(token):\n",
    "    if len(token)==5 and token[:1]=='P' and token[1:].isdigit():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_invoice_id(token):\n",
    "    if len(token)==8 and token[:3]=='INV' and token[3:].isdigit():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def recognize_my_items(tokens):\n",
    "    recognized_tokens=[]\n",
    "    for t in tokens:\n",
    "        if is_product_id(t):\n",
    "            recognized_tokens.append(\"PRODUCTID\")\n",
    "        elif is_invoice_id(t):\n",
    "            recognized_tokens.append(\"INVOICEID\")\n",
    "        else:\n",
    "            recognized_tokens.append(t)\n",
    "    return recognized_tokens\n",
    "\n",
    "tokens = recognize_my_items(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use NLTK to tag the items as nouns, verbs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged\n",
    "\n",
    "entities = nltk.ne_chunk(tagged)\n",
    "entities[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem the tokens so we can later combine terms based on their root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing stemming\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = []\n",
    "    for t in tokens:\n",
    "        stemmed_tokens.append(stemmer.stem(t))\n",
    "    return stemmed_tokens\n",
    "\n",
    "tokens = stem_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to retag the stemmed items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It did an okay job, but not perfect.  For example it thinks 'ship' is a Noun, when in this context it's a Verb. \n",
    "\n",
    "## Let's look at what these tags mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine like terms and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tokens(tokens):\n",
    "    dict = {}\n",
    "    for t in tokens:\n",
    "        count = 1\n",
    "        if t in dict.keys():\n",
    "            count = dict[t] + 1\n",
    "        dict[t] = count\n",
    "    return dict\n",
    "\n",
    "tokens_dict = combine_tokens(tagged)\n",
    "sorted(tokens_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These tokens are our features that we should use to classify the document type against ones that we already know about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
